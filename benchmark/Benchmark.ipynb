{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d457ea",
   "metadata": {},
   "source": [
    "<font size=\"5\"> <span style=\"color:white\"> ## ImageNet Zero Classification </span> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88bebc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nrequirements:\\n\\n\\n-----\\nPIL                 9.2.0\\ndatasets            2.8.0\\nmodeling_altclip    NA\\nprocessing_altclip  NA\\nrequests            2.28.1\\nsession_info        1.0.0\\ntorch               1.13.0+cu116\\ntorchvision         0.14.0+cu116\\ntqdm                4.64.1\\ntransformers        4.23.1\\n-----\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "requirements:\n",
    "\n",
    "\n",
    "-----\n",
    "PIL                 9.2.0\n",
    "datasets            2.8.0\n",
    "modeling_altclip    NA\n",
    "processing_altclip  NA\n",
    "requests            2.28.1\n",
    "session_info        1.0.0\n",
    "torch               1.13.0+cu116\n",
    "torchvision         0.14.0+cu116\n",
    "tqdm                4.64.1\n",
    "transformers        4.23.1\n",
    "-----\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbeae6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagenet_1_k (/media/khalid/data_disk/Dataset/Image/imagenet/cache/imagenet_1_k/default/1.0.0/5cd2ecbd7116e65827b8baed5493d444b466df8100f978ff07562b60f52e0ff8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007a251efe39463b8da9fb62f1da902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imagenet = load_dataset('imagenet_1K.py',\n",
    "                        cache_dir='/media/khalid/data_disk/Dataset/Image/imagenet/cache/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd371b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '{c}'}\n",
      "{'label': 'صورة سيئة لـ {c}'}\n",
      "{'label': 'صورة سيئة تحتوي على {c}'}\n",
      "{'label': 'نحت لشكل {c}'}\n",
      "{'label': 'نحت لـ {c}'}\n",
      "{'label': 'صورة ذات جوودة منخفضة لـ {c}'}\n",
      "{'label': 'صورة ذات جوودة منخفضة تحتوي {c}'}\n",
      "{'label': 'رسومات جدارية تحتوي {c}'}\n",
      "{'label': 'رسومات جدارية لـ {c}'}\n",
      "{'label': 'صورة مقتطعة تحتوي على {c}'}\n",
      "{'label': 'صورة مقتطعة لـ {c}'}\n",
      "{'label': 'تطريز {c} '}\n",
      "{'label': ' صورة يصعب فيها رؤية {c} '}\n",
      "{'label': 'صورة ساطعة لـ {c}'}\n",
      "{'label': 'صورة واضحة لـ {c}'}\n",
      "{'label': 'صورة متسخة لـ {c}'}\n",
      "{'label': 'صورة مظلمة لـ {c}'}\n",
      "{'label': 'صورة أبيض وأسود {c}'}\n",
      "{'label': '{c} في لقطة قريبة'}\n",
      "{'label': 'صورة رائعة لـ {c}'}\n",
      "{'label': 'لقطة قريبة لـ {c}'}\n",
      "{'label': 'رسم حاسوبي يحتوي {c}'}\n",
      "{'label': 'صورة مرسومة تحتوي {c}'}\n",
      "{'label': 'رسمة لـ {c}'}\n",
      "{'label': 'رسمة {c}'}\n",
      "{'label': 'رسم يحتوي {c} '}\n",
      "{'label': 'صورة بنمط البكسل لـ {c}'}\n",
      "{'label': ' صورة ساطعة {c}'}\n",
      "{'label': 'وشم {c}'}\n",
      "{'label': '{c} في الصورة'}\n",
      "{'label': 'صورة متسخة تحتوي {c}'}\n",
      "{'label': 'صورة تالفة {c}'}\n",
      "{'label': 'صورة ضبابية لـ {c}'}\n",
      "{'label': 'صورة {c}'}\n",
      "{'label': 'صورة جيدة لـ {c}'}\n",
      "{'label': 'صورة لـ {c}'}\n",
      "{'label': 'تصيير لـ {c}'}\n",
      "{'label': '{c} على شكل رسم حاسوبي ثنائي أو ثلاثي الأبعاد'}\n",
      "{'label': 'يوجد {c} واحد في الصورة'}\n",
      "{'label': 'رسم حاسوبي لـ {c}'}\n",
      "{'label': 'اوريغامي لـ {c}'}\n",
      "{'label': '{c} مصنوع عن طريق فن طي الورق'}\n",
      "{'label': '{c} في لعبة فيديو'}\n",
      "{'label': '{c} موجود في لعبة الفيديو'}\n",
      "{'label': 'رسم تقريبي لـ {c}'}\n",
      "{'label': '{c} مرسوم بالخرابيش'}\n",
      "{'label': 'صورة بفن الخرابيش لـ {c}'}\n",
      "{'label': 'لعبة {c}'}\n",
      "{'label': 'صورة يوجد فيها {c}'}\n",
      "{'label': 'رسوم متحركة لـ {c} '}\n",
      "{'label': 'صورة لعدد من {c}'}\n",
      "{'label': 'صورة يظهر فيها {c}'}\n",
      "{'label': 'صورة {c} صغير '}\n",
      "{'label': 'صورة {c} كبير'}\n",
      "{'label': '{c} يظهر في الصورة'}\n"
     ]
    }
   ],
   "source": [
    "## Convert json to jsonl\n",
    "\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('/home/khalid/Documents/github_rep/MyProjects/dataset/ar_zeroshot_classification_templates.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "datalist = []\n",
    "# Iterating through the json\n",
    "# list\n",
    "for i in data['imagenet1k']:\n",
    "    print({'label':i})\n",
    "  \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28947ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/khalid/Documents/github_rep/MyProjects/dataset/prompts.jsonl', 'w') as outfile:\n",
    "    for entry in data['imagenet1k']:\n",
    "        json.dump({'prompt':entry}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa6244d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label', 'path'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b4f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7c89a635fa395b5c\n",
      "Found cached dataset json (/home/khalid/.cache/huggingface/datasets/json/default-7c89a635fa395b5c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Using custom data configuration default-ef7ef8bb4d10e568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/khalid/.cache/huggingface/datasets/json/default-ef7ef8bb4d10e568/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7edc21ff320460c8547335256c94944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362a6f5ebae84ee8862acb4af2d50743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/khalid/.cache/huggingface/datasets/json/default-ef7ef8bb4d10e568/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "img_classes_name = load_dataset('json',\n",
    "                                split='train',\n",
    "                               data_files='/home/khalid/Documents/github_rep/MyProjects/dataset/ar_classnames.jsonl')\n",
    "\n",
    "prompts = load_dataset('json',\n",
    "                       split='train',\n",
    "                        data_files='/home/khalid/Documents/github_rep/MyProjects/dataset/prompts.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baf228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1e5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.CenterCrop(224) ,\n",
    "        #transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba1a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(ex):\n",
    "    #print(ex)\n",
    "    #ex['pixel'] = train_transforms(ex['image'])\n",
    "    ex['image'] = [train_transforms(im.convert(\"RGB\")) for im in ex['image']]\n",
    "    return ex\n",
    "\n",
    "imagenet.set_transform(convert_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db53070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [tensor([[[0.1255, 0.1216, 0.1216,  ..., 0.7804, 0.7804, 0.7765],\n",
       "           [0.1176, 0.1176, 0.1216,  ..., 0.7882, 0.7843, 0.7804],\n",
       "           [0.1176, 0.1216, 0.1294,  ..., 0.7882, 0.7882, 0.7882],\n",
       "           ...,\n",
       "           [0.0941, 0.1020, 0.0980,  ..., 0.6157, 0.6157, 0.5922],\n",
       "           [0.0941, 0.0980, 0.0980,  ..., 0.6196, 0.6157, 0.6157],\n",
       "           [0.0980, 0.1059, 0.1020,  ..., 0.6118, 0.6078, 0.6118]],\n",
       "  \n",
       "          [[0.1529, 0.1529, 0.1529,  ..., 0.7725, 0.7725, 0.7686],\n",
       "           [0.1569, 0.1569, 0.1569,  ..., 0.7804, 0.7765, 0.7765],\n",
       "           [0.1529, 0.1608, 0.1647,  ..., 0.7843, 0.7843, 0.7843],\n",
       "           ...,\n",
       "           [0.1137, 0.1216, 0.1176,  ..., 0.6157, 0.6196, 0.6078],\n",
       "           [0.1137, 0.1176, 0.1176,  ..., 0.6196, 0.6157, 0.6275],\n",
       "           [0.1216, 0.1294, 0.1255,  ..., 0.6118, 0.6118, 0.6196]],\n",
       "  \n",
       "          [[0.1216, 0.1255, 0.1373,  ..., 0.8235, 0.8235, 0.8196],\n",
       "           [0.1216, 0.1294, 0.1333,  ..., 0.8314, 0.8275, 0.8275],\n",
       "           [0.1294, 0.1294, 0.1412,  ..., 0.8275, 0.8314, 0.8392],\n",
       "           ...,\n",
       "           [0.0902, 0.0980, 0.0980,  ..., 0.6157, 0.6157, 0.6039],\n",
       "           [0.0902, 0.0980, 0.0980,  ..., 0.6157, 0.6157, 0.6275],\n",
       "           [0.1059, 0.1098, 0.1020,  ..., 0.6157, 0.6235, 0.6314]]]),\n",
       "  tensor([[[0.4157, 0.4157, 0.4196,  ..., 0.5255, 0.5176, 0.5137],\n",
       "           [0.4157, 0.4196, 0.4235,  ..., 0.5294, 0.5176, 0.5137],\n",
       "           [0.4196, 0.4235, 0.4314,  ..., 0.5333, 0.5216, 0.5137],\n",
       "           ...,\n",
       "           [0.2980, 0.3098, 0.3176,  ..., 0.1961, 0.1922, 0.1961],\n",
       "           [0.3020, 0.3137, 0.3216,  ..., 0.2157, 0.2157, 0.2235],\n",
       "           [0.3020, 0.3137, 0.3216,  ..., 0.2235, 0.2235, 0.2353]],\n",
       "  \n",
       "          [[0.4353, 0.4353, 0.4392,  ..., 0.5294, 0.5176, 0.5098],\n",
       "           [0.4353, 0.4392, 0.4431,  ..., 0.5333, 0.5216, 0.5098],\n",
       "           [0.4392, 0.4431, 0.4510,  ..., 0.5373, 0.5255, 0.5137],\n",
       "           ...,\n",
       "           [0.4078, 0.4078, 0.4039,  ..., 0.2667, 0.2471, 0.2353],\n",
       "           [0.4118, 0.4118, 0.4078,  ..., 0.2863, 0.2706, 0.2627],\n",
       "           [0.4157, 0.4157, 0.4118,  ..., 0.2941, 0.2784, 0.2745]],\n",
       "  \n",
       "          [[0.2235, 0.2157, 0.2078,  ..., 0.2118, 0.1961, 0.1843],\n",
       "           [0.2235, 0.2196, 0.2118,  ..., 0.2157, 0.2000, 0.1882],\n",
       "           [0.2275, 0.2235, 0.2196,  ..., 0.2275, 0.2118, 0.1961],\n",
       "           ...,\n",
       "           [0.1647, 0.1725, 0.1804,  ..., 0.1137, 0.1098, 0.1098],\n",
       "           [0.1608, 0.1686, 0.1765,  ..., 0.1294, 0.1255, 0.1294],\n",
       "           [0.1569, 0.1647, 0.1725,  ..., 0.1333, 0.1294, 0.1373]]])],\n",
       " 'label': [591, 297],\n",
       " 'path': ['/media/khalid/data_disk/Dataset/Image/imagenet/val_images/ILSVRC2012_val_00044989_n03485794.JPEG',\n",
       "  '/media/khalid/data_disk/Dataset/Image/imagenet/val_images/ILSVRC2012_val_00018303_n02134418.JPEG']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet['validation'].select(range(2))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f651eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset = torch.utils.data.DataLoader(imagenet['validation'],\n",
    "                                      batch_size = 16,\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2a9249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    print(i['image'].shape)\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel,AutoFeatureExtractor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\",\n",
    "                                         cache_dir='/media/khalid/data_disk/Dataset/image_and_text_pairs/cache/openai/')\n",
    "\n",
    "model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch16\",\n",
    "                                 cache_dir='/media/khalid/data_disk/Dataset/image_and_text_pairs/cache/openai/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"/media/khalid/HDD2/CLIP/clip-arbert/\",\n",
    "                                         cache_dir='/media/khalid/data_disk/Dataset/image_and_text_pairs/cache/openai/')\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/media/khalid/HDD2/CLIP/clip_arbert_finetune/\",\n",
    "                                 cache_dir='/media/khalid/data_disk/Dataset/image_and_text_pairs/cache/openai/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b45a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type altclip to instantiate a model of type clip. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "# transformers version >= 4.21.0\n",
    "from modeling_altclip import AltCLIP\n",
    "from processing_altclip import AltCLIPProcessor\n",
    "model = AltCLIP.from_pretrained(\"BAAI/AltCLIP-m9\")\n",
    "processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP-m9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d411de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd0d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd269dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "461c3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = img_classes_name['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9733a9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d3cf8447974dcc933bae4631bcd831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.4454599916934967\n"
     ]
    }
   ],
   "source": [
    "pred_sum = 0\n",
    "with torch.no_grad():\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(dataset):\n",
    "        \n",
    "        #url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        #image = Image.open(requests.get(url, stream=True).raw)\n",
    "        #print(i['image'].shape)\n",
    "        inputs = processor(text=[f'صورة {j}' for j in img_classes_name['label']],\n",
    "                        images=list(i['image']),\n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].to(device)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
    "        #inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "        \n",
    "        topk = torch.topk(probs,1)[1]\n",
    "        \n",
    "        pred_sum += (topk.t().cpu().detach() == i['label']).float().sum()\n",
    "        \n",
    "print(f\"The accuracy of the model is: {pred_sum/(len(dataset)*16)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74fa5e29823892ffbc5b7f6a77da05a68fd2fd64d19004515d09f8751f328886"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
